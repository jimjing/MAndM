\section{Motion Planning Action Service(Catherine)}

%Motivation (why did you do it, who should care about it)
%Clearly state your contributions
%Technical details of what you did, with a focus on what's novel.
%Experimental results (if you did any experiments)
%Discussion (what is interesting about the results)

%This component is divided into two subsections:
%\begin{enumerate}%[label=\thesection.\arabic*]
%\item Trajectory Planner for robot arm trajectory planning, and
%\item Trajectory Executor for robot arm trajectory execution.
%\end{enumerate}

The motion planning action service can be separated into three components as shown in Fig.~\ref{fig:plan}.

\begin{figure}[ht!]%[H]
\centering
{\includegraphics[width=0.95\columnwidth]{pics/motion_planning_flow.png}}
\caption{Motion Planning Action Service}
\label{fig:plan}
\end{figure}

The motion planning action service is an action service in ROS and based on the action types: move\_to, gripper or get\_tag, the service executes different actions and returns requested information.

\subsection{Trajectory Planner (move\_to component)}

During execution, the task planner requests the path planner to create trajectory plans for one or more robot arms. The planner aims to find a plan that (i) each arm starts at its initial configuration, and (ii) each arm ends with each robot end effector within a certain distance from the final desired workspace position. 

There are a variety of path planning algorithms developed in the community~\cite{DBLP:books/daglib/0016830} and some algorithms are adapted for finding robot arm trajectories. 
Researchers have proposed sampling-based approaches with probabilistic completeness such as the Rapidly-Exploring Random Tree (RRT) algorithm~\cite{VahrenkampBAKD09} to find trajectories for robot arms. Others have used Probabilistic RoadMaps (PRM) that work with high-dimensional spaces~\cite{KavrakiSLO96} but the algorithm requires pre-computation of the roadmap and no dynamic obstacles in the environment. In this work, we do not conduct pre-processing and the other arms can temporarily be obstacles in the environment so we are using an improved version of RRT, RRT-Connect~\cite{KuffnerL00} to find trajectories for the robot arms.


Our work leverages the existing open-source planning software for manipulation, MoveIt! \cite{moveit} with the Open Motion Planning Library (OMPL)~\cite{sucan2012the-open-motion-planning-library} to plan trajectories for the robot arms. 


\subsubsection{Objects to Avoid in Trajectory Planning}~\\
The major objects to avoid in path planning are the table and all the modules excluding the target module, as shown in Fig.~\ref{fig:realBaxter}. In this project, we have explored two different ways to model objects in the world. 

\paragraph{Octamap}\label{obstacle-octamap}
For our first attempt, we subscribe to the depth information of a Kinect facing the robot and create an octamap around the robot. An octamap is a 3-D occupany grid that builds an obstacle map around the source, in this case the Kinect, and updates over time with new information of the evolving environment. Figure~\ref{fig:octamap} shows an example of the octamap around Baxter.

Since the modules are modeled as obstacles in the octamap, to approach and grasp the module, we remove the target module from the octamap through inserting a module-like scene object into the workspace (Fig.~\ref{fig:octamap_object}). The octamap automatically subtracts any scene objects and by excluding the module object from collision check during trajectory planning, MoveIt! can find a path to the module.

\paragraph{Table object}\label{obstacle-table}
Since the environment of the workspace is given, the other way to model obstacles in the environment is simply adding a table object right in front of Baxter, as shown in Fig.~\ref{fig:table_only}. 

We compared the performance of the two approaches for modeling obstacles. It turned out clustering of modules could be a problem when using an octamap and MoveIt! could not find a plans. On the other hand, when we simply model the environment with only a table in front, the resulting plan may sometimes touch the other modules. Looking at Fig.~\ref{fig:table}, notice that the table object is a pretty good representation of the obstacle information from the octamap. Since the modules are movable in the workspace, we determined that it will be better to get a plan most of time and we go on with approach b) to model obstacles. 

%\quad
%\quad
\begin{figure*}[h]
%\begin{figure}[ht!]%[H]
\centering
\subfloat[Real Scenario of Baxter with modules on a table]{\includegraphics[width=0.35\columnwidth]{pics/baxter_real_table.png}\label{fig:realBaxter}}\hspace{0.1cm}
\subfloat[Octamap from depth data of Kinect in Rviz]{\includegraphics[width=0.35\columnwidth]{pics/baxter_with_octamap.png}\label{fig:octamap}}\hspace{0.1cm}
\subfloat[Octamap from depth data of Kinect with a module object inserted in Rviz]{\includegraphics[width=0.35\columnwidth]{pics/baxter_octamap_with_object.png}\label{fig:octamap_object}}\hspace{0.1cm}
\subfloat[Table object inserted in Rviz as obstacle]{\includegraphics[width=0.35\columnwidth]{pics/baxter_with_table_only.png}\label{fig:table_only}}\hspace{0.1cm}
\subfloat[Octamap from depth data of Kinect with a table object inserted in Rviz]{\includegraphics[width=0.35\columnwidth]{pics/baxter_with_table.png}\label{fig:table}}
\\
\caption{Modeling obstacles in trajectory planning}
\end{figure*}



\subsubsection {Transformation of Tag Pose to Gripper's Goal Pose}~\\
To plan an arm trajectory, the algorithm also requires the goal pose of the gripper to plan accordingly.  The goal pose can be separated into two components: the xyz-position of the pose ($p_{\mathrm{effector}}$) and the orientation of the pose ($q_{\mathrm{effector}}$) in the form of a quaternion.

The gripper goal pose comes from the target module pose, which consists of the position ($p_{\mathrm{tag}}$) and orientation of the module ($q_{\mathrm{tag}}$). For the goal position ($p_{\mathrm{effector}}$), it is either exactly the same as the tag position ($p_{\mathrm{tag}}$) or with a small offset ($p_{\mathrm{offset}}$) from the tag position. The goal position can be represented with the following equation:

%\begin{bmatrix}
%    \mathrm{offset}_{x} \\
%    \mathrm{offest}_{y} \\
%    \mathrm{offset}_{z}
%\end{bmatrix}
%+
%\begin{bmatrix}
%    \mathrm{tagPosition}_{x} \\
%    \mathrm{tagPosition}_{y} \\
%    \mathrm{tagPosition}_{z} \\
%\end{bmatrix}
\begin{equation}
p_{\mathrm{effector}}= p_{\mathrm{offset}}+p_{\mathrm{tag}}
\end{equation}

In this project, all the modules have z-axis pointing upwards. However, to grasp a module, the gripper should point downwards, i.e., z-axis of the gripper points downwards. To obtain the goal orientation ($q_{\mathrm{effector}}$), our system rotates the tag orientation about x-axis by $\pi$ radians, as shown in the following equation: 

\begin{equation}
q_{\mathrm{effector}}= q_{\mathrm{tag}}\: q_{\mathrm{zFlip}}\mathrm{,\ where\ } q_{\mathrm{zFlip}} =  (0}, \mathbf{i})
\end{equation}


\subsubsection {Two-step Trajectory Planning}~\\
Through testing our approach, our group have noticed that the camera information from a Kinect sensor facing the 
\begin{wrapfigure}{r}{0.45\columnwidth}
\begin{center}
\vspace{-8pt}
\includegraphics[width=0.45\columnwidth]{pics/kinect_tag_pose3_croped.png}
\caption{Differences in pose when using different sensor information.}
\label{fig:offTagPose}
\vspace{-12pt}
\end{center}
\end{wrapfigure}
robot does not give an accurate position of the modules using AprilTags recognition~\cite{Olson11},  
as the distance from Kinect to the module increases. 
The position of the modules from Kinect is off by about 5cm and it is not good enough for the arm and gripper to arrive at the module location and grasp the module. Fig.~\ref{fig:offTagPose} shows the differences of tag pose when using the Kinect sensor (tag\_4) and the left hand camera (left\_hand\_tag\_4).

With this observation, in this work, we propose a two-step trajectory planning, which consists of a `rough' planning step and a `fine' planning step, and both steps leverage RRT to conduct the planning. 

From an arbitrary location, our system first conducts a `rough' planning to have the gripper arrive at a pose that is roughly above the module using the tag information from the Kinect sensor, with the gripper pointing downwards at the module.

Once the robot arms moves to the 'rough' location, now the robot  uses the camera on its hand to obtain a better location estimate of the module. With a more accurate location, the system then conduct a `fine' planning to the grasp location of the module. 

\subsubsection{Execute Trajectory}~\\
Once a trajectory is found through `rough' planning or `fine' planning, the action service executes the trajectory and returns the result.


\subsection{Open/Close Gripper (gripper component)}
When receiving a request to open and close gripper, the component executes the command and returns the result.

\subsection{Retrieve Module Pose (get\_tag component)}
Besides responding to different action requests, the action service can also get the pose of a module and return the pose to the client. With the pose, the client can transform the pose according to his or her needs and request trajectory planning with a given pose, as shown in Fig.~\ref{fig:plan} with command move\_to and a target pose.


\subsection{Failures}
Since the trajectory planning algorithm RRT is probabilistic complete, it may not find a path given a finite time horizon. It is useful to provide feedback to the client about the status of the request or that the planning failed. In our system, it returns not only the planning result but also the current process in execution to the client so that the client is always informed.

Besides the trajectory planner may not return or execute a plan, the gripping motion and the retrieveal of module pose can also fail. All these failure information are returned to the client, as shown with the red failed arrows in Fig.~\ref{fig:plan}.


other attempts
\begin{itemize}
\item also tried broadcasting the goal location of the robot to tf and then fetch it, but then there is a lot of overhead and it will be easier doing transformation leveraging the tf.transformation module.
\item also played with the inverse kinematic library (pyKDL) before but when switch it moveit, all the inverse kinematics are handled by Moveit.
\item originally tried to generate waypoints of trajactory. To populate points on the RRT tree, but collision detection is using FCL but moveit set up the environment/scene so using moveit now.
\item create object and then removing to from collision matrix. in this case, the robot can contact the object and the octamap is cleared at the point. The downside was the filtering wasn't good around the grippers. Also adding a table gives a close approximation. 
\end{itemize}

\subsection{open and close gripper}

%--------- old %%%%


First, we plan to try two different RRT approaches and compare their performance. With RRT, there are still multiple ways to generate a trajectory plan.
One of them is to generate trajectory plan synchronously.
For example, if there are four arms available for a task, or two Baxters, then the planning could be synchronous, i.e, we plan all the arms at the same time and each node in RRT stores the joint information of the four arms. With each arm having 7 degrees of freedom (DoF), a synchronous planning has up to 28 degrees of freedom.
With this approach, assuming the workspace has no other obstacles, collision avoidance with the other arms is taken care of during the planning phase so it is unnecessary during trajectory execution.  
The trade off of synchronous planning is that one arm may wait for the others even though its trajectory is found.

Alternatively, another way would be to first create a plan each arm separately and assign priorities to each arm. An arm then replans only when its trajectory intersects with the plan of another arm with a higher priority. Compared with the synchronous approach, each planning contains 7 degrees of freedom but re-plannings of trajectory can go up to (n-1) times, with n being the number of arms. 

Second, we plan to create our own version of RRT planner and also utilize off-the-shelf library with RRT such as the Open Motion Planning library (OMPL)~\cite{sucan2012the-open-motion-planning-library} to find out the one with better performance.
Compared with creating our own RRT planner, OMPL has lots of planning algorithms available, but most examples of OMPL work with single robot or arm and planning for multiple arms simultaneously with OMPL may be unfeasible. We plan to learn more about OMPL and then decide if we are creating our own RRT planner or we are using OMPL to build our planner.

Finally, RRT is only a template for planning and we need to complete the template with ways to propagate a trajectory and smoothen the resulting trajectory. 
During the planning phrase, we plan to optimize the final trajectory by reducing the difference in joint angles between two nodes in the RRT tree. We use inverse kinematics find robot joint angles given a desired location of the end effector.


%To conduct a path planning of the robot arms, the planner takes in:

%\begin{itemize}
%\item the number of arms we are planning, 
%\item the starting joint configurations of each arm, and
%\item the final workspace positions of robot end effectors.
%\end{itemize}



%The action server lib is can respond to five different action requests
%\begin{itemize}
%\item open gripper
%\item close gripper
%\item go to a pose that the hand camera can see the tag, with a fix pose pre-given
%\item go to a tag
%\item get tag pose
%\end{itemize}



